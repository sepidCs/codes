{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_reduced(tensor_data,factors,reduced_k,num_comp):\n",
    "# #toDo\n",
    "#     f0=factors[0][:,:reduced_k]\n",
    "#     f1=factors[1][:,:num_comp]\n",
    "\n",
    "#     new_data=tl.tenalg.mode_dot(tensor_data, f1.T, mode=1, transpose=False)\n",
    "#     new_data=tl.tenalg.mode_dot(new_data, f0.T, mode=0, transpose=False)\n",
    "#     print(\"data size is\",new_data.shape)\n",
    "#     n_dim=new_data.shape\n",
    "#     new_data=new_data.reshape(n_dim[0]*n_dim[1],n_dim[2]).T\n",
    "#     print(\"unfolded data size is\",new_data.shape)\n",
    "#     return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "این برنامه را چون انرژی که استفاده میکنه کارا نیست رها میکنیم و با \n",
    "QR\n",
    " که دقیق تره ادامه میدیم\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorizing(k,data):\n",
    "    #this func transform the 2D data to a 3D tensor\n",
    "    # first dimension is timeseries samples , second is features and third dimension is time\n",
    "    no_target_data=data.drop(columns=['DEMAND'])\n",
    "    tensor_data=np.zeros((k,no_target_data.shape[1],data.shape[0]-k+1))\n",
    "    for i in range(data.shape[0]-k+1):\n",
    "        tensor_data[:,:,i]=no_target_data.iloc[i:i+k].to_numpy()\n",
    "    print (\"tensorized data shape is\",tensor_data.shape)\n",
    "    return tensor_data\n",
    "\n",
    "def hosvd(tensor_data):\n",
    "    core, factors = tucker(tensor_data, rank=None,random_state=17)\n",
    "    # print(\"core rank is\",core.shape)\n",
    "    return core,factors\n",
    "\n",
    "\n",
    "\n",
    "def data_reduced(tensor_data,reduced_window_size,num_comp):\n",
    "    #inputs:\n",
    "    # tensor_data is the data which we mean to reduce from first and second dimension\n",
    "    # reduced_window_size is size of first dimension of the output tensor\n",
    "    # num_comp is number of features of output\n",
    "    #output:\n",
    "    # a 2D array which obtained by unfolding a tensor of size (reduced_window_size,num_comp,tensor_data.shape[2])\n",
    "    core, factors = tucker(tensor_data, rank=[reduced_window_size,num_comp,tensor_data.shape[2]],random_state=17)\n",
    "    f0=factors[0]\n",
    "    f1=factors[1]\n",
    "    # print(\"u1\",f1)\n",
    "    new_data=tl.tenalg.mode_dot(tensor_data, f1.T, mode=1, transpose=False)\n",
    "    new_data=tl.tenalg.mode_dot(new_data, f0.T, mode=0, transpose=False)\n",
    "    print(\"reduced data size is\",new_data.shape)\n",
    "    n_dim=new_data.shape\n",
    "    new_data=new_data.reshape(n_dim[0]*n_dim[1],n_dim[2]).T\n",
    "    print(\"unfolded data size is\",new_data.shape)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def reg_err(data,y,testSize):\n",
    "    print(data.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = testSize,shuffle=False, random_state = 42)\n",
    "    # print(X_test==data[-500:,:])\n",
    "    # print(X_test.shape==data[-500:,:].shape)\n",
    "\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X_train, y_train)\n",
    "    y_pred = linear_regressor.predict(X_test)\n",
    "    linear_mae=mean_absolute_error(y_test,y_pred)\n",
    "    linear_mse=mean_squared_error(y_test,y_pred)\n",
    "    print(\"linear regression\")\n",
    "    print(\"mae ={:.2f}\".format(linear_mae))\n",
    "    print(\"mse ={:.2f}\".format(linear_mse))\n",
    "\n",
    "    xgb_regressor = xg.XGBRegressor()\n",
    "    xgb_regressor.fit(X_train, y_train)\n",
    "    y_pred = xgb_regressor.predict(X_test)\n",
    "    xgb_mae=mean_absolute_error(y_test,y_pred)\n",
    "    xgb_mse=mean_squared_error(y_test,y_pred)\n",
    "    print(\"xgboost regression\")\n",
    "    print(\"mae ={:.2f}\".format(xgb_mae))\n",
    "    print(\"mse ={:.2f}\".format(xgb_mse))\n",
    "\n",
    "\n",
    "def energy(core,tol_u1,tol_u2):\n",
    "    ''' calculating energy of u1 and u2 (the first and second factor of hosvd decomposition)\n",
    "    using core tensor \n",
    "    output:\n",
    "    num_comp= number of features which meet the energy tol (tol_u1) condition \n",
    "    reduced_window_size = size of window which meet the tol_u2 condition'''\n",
    "\n",
    "    # norm of each slice through first order\n",
    "    norm_arr=[np.linalg.norm(core[i,:,:]) for i in range(core.shape[0])]\n",
    "    # energy list of u1\n",
    "    e_u1=[sum(norm_arr[:i])/sum(norm_arr) for i in range(len(norm_arr))]\n",
    "    # print(\"u1_energy=\",e_u1)\n",
    "    # select first energy which meet the condition\n",
    "    reduced_window_size=next(e_u1.index(x) for x in e_u1 if x>=tol_u1) +1\n",
    "\n",
    "    # norm of each slice through second order\n",
    "    norm_arr=[np.linalg.norm(core[:,i,:]) for i in range(core.shape[1])]\n",
    "    #energy list of U2 (features)\n",
    "    e_u2=[sum(norm_arr[:i])/sum(norm_arr)  for i in range(len(norm_arr))]\n",
    "    # print(\"u2_energy=\",e_u2)\n",
    "    # select first energy which meet the condition\n",
    "    num_comp=next(e_u2.index(x) for x in e_u2 if x>=tol_u2) +1 \n",
    "\n",
    "    return reduced_window_size,num_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data size is (46968, 11)\n",
      "subsample size is (2000, 11)\n"
     ]
    }
   ],
   "source": [
    "# import timeseries data\n",
    "dateparse = lambda x: pd.to_datetime(x, format = \"%Y-%m-%d %H:00:00\", errors='coerce')\n",
    "data= pd.read_csv('..\\kaggle dataset\\dataframes.csv' ,index_col=[\"datetime\"], parse_dates=['datetime'], date_parser=dateparse, skipinitialspace=True)\n",
    "print(\"original data size is\",data.shape)\n",
    "# select a subsample of data\n",
    "data=data.iloc[:2000]\n",
    "print(\"subsample size is\",data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=50  \n",
    "testSize=500 #for test train split\n",
    "tol_u1=.9\n",
    "tol_u2=.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorized data shape is (50, 10, 1951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User.MORADI-PC\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorly\\decomposition\\_tucker.py:60: Warning: No value given for 'rank'. The decomposition will preserve the original size.\n",
      "  warnings.warn(message, Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_window_size= 10  num_comp= 4\n",
      "reduced data size is (10, 4, 1951)\n",
      "unfolded data size is (1951, 40)\n"
     ]
    }
   ],
   "source": [
    "tensor_data=tensorizing(window_size,data)\n",
    "# full rank hosvd (core is used for energy calculation)\n",
    "core,_=hosvd(tensor_data)\n",
    "#optimized number of reduced_window_size,num_comp based on energy\n",
    "reduced_window_size, num_comp = energy(core,tol_u1,tol_u2)\n",
    "# reduced_window_size,num_comp=20,5\n",
    "print(\"reduced_window_size=\",reduced_window_size,\" num_comp=\",num_comp)\n",
    "# 2D dataset based on reduced hosvd \n",
    "new_data=data_reduced(tensor_data,reduced_window_size,num_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new method errors:\n",
      "(1951, 40)\n",
      "linear regression\n",
      "mae =61.67\n",
      "mse =7226.89\n",
      "xgboost regression\n",
      "mae =62.34\n",
      "mse =6929.85\n"
     ]
    }
   ],
   "source": [
    "y_newdata=data.iloc[window_size-1:,10]\n",
    "print (\"new method errors:\")\n",
    "reg_err(new_data,y_newdata,testSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca method errors:\n",
      "(2000, 4)\n",
      "linear regression\n",
      "mae =50.53\n",
      "mse =5874.98\n",
      "xgboost regression\n",
      "mae =68.37\n",
      "mse =9148.83\n"
     ]
    }
   ],
   "source": [
    "no_target_data=data.drop(columns=['DEMAND'])\n",
    "pca = PCA(n_components=num_comp).fit(no_target_data)\n",
    "X=pca.transform(no_target_data)\n",
    "y_pca=data.iloc[:,10]\n",
    "print (\"pca method errors:\")\n",
    "reg_err(X,y_pca,testSize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb9bb2fa3c6d437e42749e79e4a1d55d8d222e6cc8216a87b1a35470410f9d89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
