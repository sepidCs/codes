{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_files_with_label(session_number, subject_id, desired_label):\n",
    "    base_dir=r\"D:\\thesis-main\\codes\\database\\seed-iv\\output_csv\\output_csv\"\n",
    "    # base_dir = r\"D:\\code\\sepide\\codes\\data\\seed-iv\\output_csv\"\n",
    "    session_name = \"session\" + str(session_number)\n",
    "    # Convert subject_id to string if needed\n",
    "    subject_str = str(subject_id)\n",
    "\n",
    "    # Build the subject folder path\n",
    "    subject_dir = os.path.join(base_dir, session_name, subject_str)\n",
    "\n",
    "    # Collect all CSV files in this subject's folder\n",
    "    csv_pattern = os.path.join(subject_dir, \"*.csv\")\n",
    "    all_csv_files = sorted(glob.glob(csv_pattern))\n",
    "\n",
    "    # We will look for filenames of the form: SOMETXT_eeg<number>_<label>.csv\n",
    "    # Example: cz_eeg1_2.csv means the EEG number is 1, label is 2.\n",
    "    # We'll parse these with a regular expression.\n",
    "    regex = re.compile(r\"(.*_eeg)(\\d+)_(\\d+)\\.csv\")\n",
    "\n",
    "    matched_files = []\n",
    "\n",
    "    for csv_file in all_csv_files:\n",
    "        filename = os.path.basename(csv_file)\n",
    "\n",
    "        match = regex.match(filename)\n",
    "        if match:\n",
    "            # Extract the eeg number and label from the filename\n",
    "            eeg_number = int(match.group(2))  # e.g. 1\n",
    "            file_label = int(match.group(3))  # e.g. 2\n",
    "\n",
    "            # Check if this file has the desired label\n",
    "            if file_label == desired_label:\n",
    "                matched_files.append((csv_file, eeg_number))\n",
    "\n",
    "    # Sort the matched files by the EEG number\n",
    "    matched_files.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Read and store data in a list of DataFrames\n",
    "    dataframes = []\n",
    "    for csv_file, eeg_number in matched_files:\n",
    "        df = pd.read_csv(csv_file, header=None)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    desired_columns = [14, 22, 23, 24, 30, 31, 32, 33, 39, 40, 41, 49]\n",
    "    df_filtered = df.iloc[:, desired_columns]\n",
    "    new_column_names = [f\"Ch{col_idx}\" for col_idx in desired_columns]\n",
    "    df_filtered.columns = new_column_names\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SICE\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def normr(a, replace_zero_rows=True):\n",
    "    a = normalize(a, axis=1, norm=\"l2\")\n",
    "    if replace_zero_rows:\n",
    "        for i in range(a.shape[0]):\n",
    "            if np.sum(np.abs(a[i, :])) == 0:\n",
    "                a[i, :] = 1 / np.sqrt(a.shape[1])\n",
    "    return a\n",
    "\n",
    "\n",
    "def window_moving(data_without_target, window_size, shift):\n",
    "\n",
    "    n = len(data_without_target)\n",
    "    win_index_dic = []\n",
    "    start_id = 0\n",
    "\n",
    "    while True:\n",
    "        end_id = start_id + window_size\n",
    "\n",
    "        # Stop if this window would go past the end of the DataFrame\n",
    "        if end_id > n:\n",
    "            break\n",
    "\n",
    "        # Record this full window\n",
    "        window = {\n",
    "            \"start_index\": start_id,\n",
    "            \"end_index\": end_id - 1,  # end index is inclusive\n",
    "        }\n",
    "        win_index_dic.append(window)\n",
    "\n",
    "        # Move forward by 'shift'\n",
    "        start_id += shift\n",
    "\n",
    "        # If we've moved beyond the dataset, we're done\n",
    "        if start_id >= n:\n",
    "            break\n",
    "\n",
    "    num_windows = len(win_index_dic)\n",
    "    return win_index_dic, num_windows\n",
    "\n",
    "\n",
    "def cosine_sim(data_without_target, windows, num_windows):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    data_without_target = pandas dataset without target column\n",
    "    windows = list of dictionaries which contain start and end index of each window in order\n",
    "    num_windows = total number of segments\n",
    "    output:\n",
    "    sim :i-th frontal slice of sim tensor is a similarity matrix of i-th window's features\n",
    "    \"\"\"\n",
    "    sim = np.zeros(\n",
    "        (data_without_target.shape[1], data_without_target.shape[1], num_windows)\n",
    "    )\n",
    "    for i in range(num_windows):\n",
    "        segmented_window = data_without_target.iloc[\n",
    "            windows[i][\"start_index\"] : windows[i][\"end_index\"] + 1, :\n",
    "        ].T.to_numpy()\n",
    "        temp = normr(segmented_window)\n",
    "        cos_matrix = np.abs(temp.dot(temp.T))\n",
    "        # cos_matrix=cosine_similarity(segmented_window,dense_output=False)\n",
    "        sim[:, :, i] = cos_matrix - np.diag(np.diag(cos_matrix))\n",
    "    return sim\n",
    "\n",
    "\n",
    "def buildHistogram(matrix):\n",
    "    plt.hist(matrix, bins=30)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def H_matrix(sim):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    sim :i-th frontal slice of tensor is a similarity matrix of i-th window's features\n",
    "    output:\n",
    "    H =  the second order correlation matrix ( of [i,j,:] fibers similarity)\n",
    "    from feature similarity tensor\"\"\"\n",
    "\n",
    "    [n1, n2, n3] = sim.shape\n",
    "    feature_similarity_matrix = sim.reshape(n1 * n2, n3)\n",
    "    temp = normr(feature_similarity_matrix, False)\n",
    "\n",
    "    cos_matrix = np.abs(temp.dot(temp.T))\n",
    "    Q = cos_matrix - np.diag(np.diag(cos_matrix))\n",
    "    T = np.zeros(Q.shape)\n",
    "    T[:, np.sum(np.abs(Q), 0) == 0] = 1 / Q.shape[0]\n",
    "    Q = Q + T\n",
    "    Q = normalize(Q, axis=0, norm=\"l1\")\n",
    "    return Q\n",
    "\n",
    "\n",
    "def page_rank_vec(sim):\n",
    "    \"\"\"\n",
    "    Computes the PageRank vector using the power iteration method.\n",
    "\n",
    "    Args:\n",
    "    sim (numpy.ndarray): Similarity matrix.\n",
    "    tol (float): Tolerance for convergence.\n",
    "    alpha (float): Damping factor.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: PageRank vector.\n",
    "    \"\"\"\n",
    "    max_iter = 1000\n",
    "    tol = 10**-7\n",
    "    alpha = 0.85\n",
    "    Q = H_matrix(sim)\n",
    "    n = Q.shape[0]\n",
    "    e = np.ones((n, 1))\n",
    "    A = alpha * Q + (1 - alpha) / n * (e.dot(e.T))\n",
    "\n",
    "    pr = np.ones(n) / n  # Initial PageRank vector\n",
    "    for _ in range(max_iter):\n",
    "        pr_new = A.dot(pr)\n",
    "        pr_new /= np.sum(pr_new)  # Normalize the PageRank vector\n",
    "        if np.linalg.norm(pr_new - pr, 1) < tol:\n",
    "            break\n",
    "        pr = pr_new\n",
    "\n",
    "    return pr\n",
    "\n",
    "\n",
    "def page_rank_to_weight_matrix_seed_iv(pr):\n",
    "    # pr[pr<np.mean(pr)]=0\n",
    "    number_of_feature = int(np.sqrt(len(pr)))\n",
    "    pr_reshaped = pr.reshape(number_of_feature, number_of_feature)\n",
    "    return pr_reshaped\n",
    "\n",
    "\n",
    "def weight_matrix_seed_iv(data_without_target, window_size, shift):\n",
    "    win_index_dic, num_windows = window_moving(data_without_target, window_size, shift)\n",
    "    feature_similarity_tensor = cosine_sim(\n",
    "        data_without_target, win_index_dic, num_windows\n",
    "    )  # sim\n",
    "    page_rank_vector = page_rank_vec(feature_similarity_tensor)\n",
    "    pr_reshaped = page_rank_to_weight_matrix_seed_iv(page_rank_vector)\n",
    "    return pr_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_histogram(weight_matrix, bins=50):\n",
    "\n",
    "    # Convert to NumPy array if needed\n",
    "    weights = np.array(weight_matrix).flatten()\n",
    "\n",
    "    # Create the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(weights, bins=bins, color=\"blue\", alpha=0.7, edgecolor=\"black\")\n",
    "\n",
    "    plt.title(\"Distribution of Weight Matrix Values\")\n",
    "    plt.xlabel(\"Weight Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Show the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_df_to_full_windows(df, window_size=800, shift=200):\n",
    "\n",
    "    n = len(df)\n",
    "\n",
    "    # If there's not enough rows for even one full window, return empty\n",
    "    if n < window_size:\n",
    "        return df.iloc[0:0]  # empty DataFrame with same columns\n",
    "\n",
    "    last_full_end = ((n - window_size) // shift) * shift + window_size\n",
    "\n",
    "    # Safety check: if last_full_end is non-positive, return empty\n",
    "    if last_full_end <= 0:\n",
    "        return df.iloc[0:0]\n",
    "\n",
    "    # Return a truncated DataFrame from the start up to 'last_full_end' rows\n",
    "    truncated_df = df.iloc[:last_full_end]\n",
    "    return truncated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure these functions exist/imported:\n",
    "#   get_eeg_files_with_label(...)\n",
    "#   truncate_df_to_full_windows(...)\n",
    "#   weight_matrix_seed_iv(...)\n",
    "\n",
    "window_size = 800\n",
    "shift = 200\n",
    "desired_label_=3\n",
    "output_dir = r\"D:\\code\\sepide\\codes\\experiments\\seed-iv-weight-matrix\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    # 1. Retrieve and truncate data for session 1\n",
    "    dataset_1 = get_eeg_files_with_label(\n",
    "        session_number=1, subject_id=i, desired_label=desired_label_\n",
    "    )\n",
    "    truncated_dataset_1 = truncate_df_to_full_windows(dataset_1, window_size, shift)\n",
    "\n",
    "    # 2. Retrieve and truncate data for session 2\n",
    "    dataset_2 = get_eeg_files_with_label(\n",
    "        session_number=2, subject_id=i, desired_label=desired_label_\n",
    "    )\n",
    "    truncated_dataset_2 = truncate_df_to_full_windows(dataset_2, window_size, shift)\n",
    "\n",
    "    # 3. Retrieve and truncate data for session 3\n",
    "    dataset_3 = get_eeg_files_with_label(\n",
    "        session_number=3, subject_id=i, desired_label=desired_label_\n",
    "    )\n",
    "    truncated_dataset_3 = truncate_df_to_full_windows(dataset_3, window_size, shift)\n",
    "\n",
    "    # 4. Concatenate data from all three sessions\n",
    "    dataset = pd.concat(\n",
    "        [truncated_dataset_1, truncated_dataset_2, truncated_dataset_3], axis=0\n",
    "    )\n",
    "\n",
    "    # 5. Generate a weight matrix (NumPy array) for the combined dataset\n",
    "    weight_matrix = weight_matrix_seed_iv(dataset, window_size, shift)\n",
    "    plot_weight_histogram(weight_matrix, bins=50)\n",
    "\n",
    "    # 6. Convert the weight matrix to a DataFrame\n",
    "    df_weight_matrix = pd.DataFrame(weight_matrix)\n",
    "\n",
    "    # 7. Construct the output CSV filename for the current subject\n",
    "    csv_filename = f\"weight_matrix_subject_{i}.csv\"\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "\n",
    "    # 8. Save the DataFrame to CSV (no headers, no index)\n",
    "    df_weight_matrix.to_csv(csv_path, header=False, index=False)\n",
    "\n",
    "    print(f\"Saved weight matrix for subject {i} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight_histogram(weight_matrix, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
